---
layout: about
title:
permalink: /
subtitle: #<a href='https://engineering.linkedin.com/'>LinkedIn Inc.</a>

profile:
  align: right
  image: vk_prof_pic.jpeg
  image_circular: false # crops the image to make it circular
  more_info: 
    # >
    # <p>555 your office number</p>
    # <p>123 your address street</p>
    # <p>Your City, State 12345</p>

news: false # includes a list of news
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

Hi :wave:, I'm a Senior Software Engineer, ML in the Foundation Models and Data team at <a href='https://www.linkedin.com/blog/engineering/artificial-intelligence'>LinkedIn AI</a>. My current research focuses on post-training techniques for <a href='https://arxiv.org/pdf/2501.16450v1'>360Brew</a>, which is LinkedIn's foundation model for recommendation.

Previously, I worked with [Prof. Joan Bruna](https://cims.nyu.edu/~bruna/) (NYU Courant), [Prof. Yaoqing Yang](https://sites.google.com/site/yangyaoqingcmu/) (Dartmouth) and [Prof. Rahul Mazumder](https://www.mit.edu/~rahulmaz/) (MIT) on topics related to the learning dynamics of neural networks. I also spent time at IBM as a TensorFlow contributor and TensorFlow-IO maintainer. I did my MSc in Computer Science at NYU Courant, and B.Tech in Electronics and Communication Engineering from IIT Guwahati.

## Research
I'm primarily interested in understanding the computational aspects of learning in neural networks, and using these insights to develop novel modeling techniques.

- **Foundation Models:** Chain-of-Thought reasoning, In-Context Learning and Multi-Task Learning in Transformer based LLMs.
- **Learning Dynamics:** Understanding the interplay between feature evolution, weight matrix spectra and generalization in shallow and deep neural networks.
- **Efficient Training/Inference:** Developing low-level (randomized) algorithms for training and inference of large scale LLMs/GNNs.
<br/><br/>

## Whats New

- **[2025.05]** :tada: The ["CoT-ICL Lab"](https://arxiv.org/abs/2502.15132) paper has been accepted to ACL Main 2025!
- **[2025.04]** :thinking: ["Can Kernel Methods Explain How the Data Affects Neural Collapse?"](https://openreview.net/forum?id=MbF1gYfIlY) has been accepted to TMLR!
- **[2025.02]** :fire: Our [tech-report](https://arxiv.org/abs/2502.14305) on training and deploying production-grade LLMs at LinkedIn is available on arxiv!
- **[2025.01]** :rocket: The 360Brew foundation model [tech-report](https://arxiv.org/pdf/2501.16450v1) is available on arxiv!
- **[2024.10]** :robot: Our [tech-report](https://arxiv.org/abs/2502.14305) on [Liger-Kernels](https://github.com/linkedin/Liger-Kernel) is now  available on arxiv. ([GPU Mode](https://www.youtube.com/watch?v=gWble4FreV4)/[Lightning AI](https://www.youtube.com/watch?v=3H_aw6o-d9c)/[AMD+Embedded LLM](https://embeddedllm.com/blog/cuda-to-rocm-portability-case-study-liger-kernel)/[Blog](https://www.linkedin.com/blog/engineering/open-source/liger-kernel-open-source-ecosystem-for-efficient-llm-training))
<br/><br/>

## Academic Service
- **Conference Reviewer:** NeurIPS 2024, ICML 2025
- **Journal Reviewer:** : IEEE Transactions on Cybernetics, IEEE Access, IEEE Transactions on Industrial Informatics, TMLR
<br/><br/>