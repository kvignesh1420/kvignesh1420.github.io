---
layout: about
title:
permalink: /
subtitle: #<a href='https://engineering.linkedin.com/'>LinkedIn Inc.</a>

profile:
  align: right
  image: vk_prof_pic.jpeg
  image_circular: false # crops the image to make it circular
  more_info: 
    # >
    # <p>555 your office number</p>
    # <p>123 your address street</p>
    # <p>Your City, State 12345</p>

news: false # includes a list of news
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

Hi :wave:, I'm a first year CS Ph.D. student at Stanford University. I'm primarily interested in understanding the computational aspects of learning in neural networks, and using these insights to develop novel modeling techniques.

- **Foundation Models:** Chain-of-Thought reasoning, In-Context Learning and Multi-Task Learning in Transformer based LLMs.
- **Learning Dynamics:** Understanding feature evolution, weight matrix spectra and generalization in neural networks.
- **Efficient Training/Inference:** Developing low-level (randomized) algorithms for training and inference of large scale LLMs/GNNs.

Previously, I was a senior ML engineer at <a href='https://www.linkedin.com/blog/engineering/artificial-intelligence'>LinkedIn AI</a> building recommendation foundation models. I earned my MSc in Computer Science at NYU Courant, where I worked with [Prof. Joan Bruna](https://cims.nyu.edu/~bruna/) on the Neural Collapse phenomenon. I have also contributed to TensorFlow and served as a maintainer for TensorFlow-IO during my time at IBM. I hold a B.Tech in Electronics and Communication Engineering from IIT Guwahati.
<br/><br/>

## Whats New

- **[2025.09]** :school_satchel: Started Ph.D. at Stanford!
- **[2025.09]** :trophy: Our work on training and deploying production-grade LLMs at LinkedIn is accepted for an Oral presentation at EMNLP (Industry Track).
- **[2025.08]** :round_pushpin: ["From Spikes to Heavy Tails: Unveiling the Spectral Evolution of Neural Networks"](https://openreview.net/forum?id=DJHB8eBUnt) has been accepted to TMLR!
- **[2025.06]** :tiger: Our ["Liger Kernel"](https://arxiv.org/abs/2410.10989) paper has been accepted to CODE ML Workshop, ICML 2025!
- **[2025.05]** :tada: The ["CoT-ICL Lab"](https://arxiv.org/abs/2502.15132) paper has been accepted to ACL Main 2025!
- **[2025.04]** :thinking: ["Can Kernel Methods Explain How the Data Affects Neural Collapse?"](https://openreview.net/forum?id=MbF1gYfIlY) has been accepted to TMLR!
- **[2025.01]** :rocket: The [360Brew foundation model](https://arxiv.org/pdf/2501.16450v1) tech-report is available on arxiv!
- **[2024.10]** :robot: Our [tech-report](https://arxiv.org/abs/2502.14305) on [Liger-Kernels](https://github.com/linkedin/Liger-Kernel) is now  available on arxiv. ([GPU Mode](https://www.youtube.com/watch?v=gWble4FreV4)/[Lightning AI](https://www.youtube.com/watch?v=3H_aw6o-d9c)/[AMD+Embedded LLM](https://embeddedllm.com/blog/cuda-to-rocm-portability-case-study-liger-kernel)/[Blog](https://www.linkedin.com/blog/engineering/open-source/liger-kernel-open-source-ecosystem-for-efficient-llm-training))
<br/><br/>

## Academic Service
- **Conference Reviewer:** NeurIPS 2024, ICML 2025
- **Journal Reviewer:** IEEE Transactions on Cybernetics, IEEE Access, IEEE Transactions on Industrial Informatics, TMLR
<br/><br/>